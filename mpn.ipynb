{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI를 활용한 포트폴리오 최적화 모델 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "포트폴리오 최적화(Portfolio Optimization)는 주요한 계량적 투자방법 중 하나입니다. 대표적인 포트폴리오 최적화 방법으로는 과거 수익율의 평균과 분산을 활용하여 원하는 자산별 투자비율을 설정을 통해 위험대비 수익율을 최대화 하는 방법이 대표적입니다. 이외에도 과거의 정보만을 가지고 판단하는 방법의 단점을 개선하여 미래에 대한 전망을 반영하여 포트폴리오를 최적화하는 블랙리터만(Black-Litterman) 방법이 있습니다.\n",
    "\n",
    "최근들어서는 딥러닝과 같은 인공지능 기술을 활용하는 방법이 많이 연구되고 있습니다. 여기서는 전통적인 마코비츠(Markowitz) 방법과 딥러닝을 접목하여 포트폴리오를 최적화 하는 모델을 구현하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 준비 단계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필요 라이브러리 로드 및 환경변수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import referenceBL\n",
    "import datetime\n",
    "import importlib\n",
    "importlib.reload(referenceBL)\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rc('font', family=\"Malgun Gothic\")\n",
    "plt.rc('axes', unicode_minus=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloomberg 가격 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로딩하려는 대상 티커 리스트를 가져온다.\n",
    "file_tickers = 'tickers.csv'\n",
    "tickers = pd.read_csv(file_tickers, header=None)\n",
    "tickers = tickers[0].tolist()\n",
    "tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 티커별로 이름, 통화, 거래소, 종목유형, 생성일, 수정일을 저장할 데이터프레임을 생성한다.\n",
    "blp = referenceBL.BLPInterface()\n",
    "master_table = []\n",
    "for ticker in tickers:\n",
    "    temp = []\n",
    "    try:\n",
    "        currency = blp.referenceRequest(securities=ticker, fields='CRNCY')\n",
    "        temp.append(currency)\n",
    "        exchange = blp.referenceRequest(securities=ticker, fields='CDR_EXCH_CODE')\n",
    "        temp.append(exchange)\n",
    "        description = blp.referenceRequest(securities=ticker, fields='name')\n",
    "        temp.append(description)\n",
    "        sec_type = blp.referenceRequest(securities=ticker, fields='SECURITY_TYP')\n",
    "        temp.append(sec_type)\n",
    "        created_date = datetime.datetime.now()\n",
    "        temp.append(created_date)\n",
    "        last_updated_date = datetime.datetime.now()\n",
    "        temp.append(last_updated_date)\n",
    "        master_table.append(temp)\n",
    "    except Exception:\n",
    "        print(\"{} was not completed for master table\".format(ticker))\n",
    "        pass\n",
    "blp.close()\n",
    "master_df = pd.DataFrame(master_table, index=tickers,\n",
    "                            columns=['CURRENCY', 'EXC_CODE', 'DESCRIPTION',\n",
    "                                    'TYPE', 'CREATED_DATE', 'UPDATED_DATE'])\n",
    "master_df['BLCODE'] = tickers\n",
    "master_df.index.names = ['EQID']\n",
    "save_id_df = 'master_df.csv'\n",
    "master_df.to_csv(save_id_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제로 historicalRequest를 통해 데이터를 가져온다.\n",
    "blp = referenceBL.BLPInterface()\n",
    "today = datetime.datetime.today()\n",
    "today = '{:02d}{:02d}{:02d}'.format(today.year, today.month, today.day)\n",
    "data_table_all = []\n",
    "for ticker in tickers:\n",
    "    data_name = blp.referenceRequest(ticker, 'NAME')\n",
    "    data_table = blp.historicalRequest(\n",
    "        securities=ticker, \n",
    "        fields=['PX_Last'], \n",
    "        startDate='20000103', \n",
    "        endDate=today)\n",
    "    data_table.columns = [ticker]\n",
    "    data_table_all.append(data_table)\n",
    "    print(ticker)\n",
    "\n",
    "blp.close()\n",
    "data = pd.concat(data_table_all, axis=1)\n",
    "\n",
    "save_id_data = 'data.csv'\n",
    "data.to_csv(save_id_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = master_df['DESCRIPTION'].values\n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리(N/A 제거)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./data 폴더의 BloombergMain.py(융기원 코드)를 통해 생성한 csv파일을 불러온다.\n",
    "data = pd.read_csv('data.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    # 일자별 종가인 dataframe을 받아서 nan값 제거, linear interpolation을 한후 로그일간수익율로 변환하여 반환한다.\n",
    "    df = data.dropna(thresh=4)\n",
    "    df = df.interpolate(method='linear', limit_direction='forward')  # 연휴에 따른 급격한 변화를 smoothing해주기 위해 interpolation\n",
    "    df = df.dropna()\n",
    "    dr = np.log(df).diff(1).dropna()\n",
    "    return dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocessing(data)\n",
    "df.to_csv('data_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 로그수익율 파일을 로드해서 시작하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# tensorflow의 래핑 라이브러리인 keras에서 본 튜토리얼에 사용할 기능들\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Input\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rc('font', family=\"Malgun Gothic\")\n",
    "plt.rc('axes', unicode_minus=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2011-01-31', '2011-02-01', '2011-02-02', '2011-02-03',\n",
      "               '2011-02-04', '2011-02-07', '2011-02-08', '2011-02-09',\n",
      "               '2011-02-10', '2011-02-11',\n",
      "               ...\n",
      "               '2022-04-04', '2022-04-05', '2022-04-06', '2022-04-07',\n",
      "               '2022-04-08', '2022-04-11', '2022-04-12', '2022-04-13',\n",
      "               '2022-04-14', '2022-04-18'],\n",
      "              dtype='datetime64[ns]', name='Date', length=2823, freq=None)\n"
     ]
    }
   ],
   "source": [
    "# 로그수익율로 전처리 완료된 파일을 불러온다.\n",
    "dr = pd.read_csv('data_preprocessed.csv', index_col=0, parse_dates=True)\n",
    "# dr = pd.read_csv('https://github.com/suhan-jung/portfolio_optimization/blob/master/data/data_preprocessed.csv', index_col=0, parse_dates=True)\n",
    "print(dr.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DBE US Equity', 'IAU US Equity', 'SLV US Equity', 'AIA US Equity',\n",
       "       'VPL US Equity', 'VGK US Equity', 'VWO US Equity',\n",
       "       'VXUS US Equity', 'RWO US Equity', 'VOO US Equity', 'VO US Equity',\n",
       "       'VB US Equity', 'SCHH US Equity', 'EMLC US Equity',\n",
       "       'AGG US Equity'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers = dr.columns.values\n",
    "tickers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Test data 분리 - 8 : 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2258\n",
      "train data index\n",
      "DatetimeIndex(['2011-01-31', '2011-02-01', '2011-02-02', '2011-02-03',\n",
      "               '2011-02-04', '2011-02-07', '2011-02-08', '2011-02-09',\n",
      "               '2011-02-10', '2011-02-11',\n",
      "               ...\n",
      "               '2020-01-07', '2020-01-08', '2020-01-09', '2020-01-10',\n",
      "               '2020-01-13', '2020-01-14', '2020-01-15', '2020-01-16',\n",
      "               '2020-01-17', '2020-01-21'],\n",
      "              dtype='datetime64[ns]', name='Date', length=2258, freq=None)\n",
      "test data index\n",
      "DatetimeIndex(['2020-01-22', '2020-01-23', '2020-01-24', '2020-01-27',\n",
      "               '2020-01-28', '2020-01-29', '2020-01-30', '2020-01-31',\n",
      "               '2020-02-03', '2020-02-04',\n",
      "               ...\n",
      "               '2022-04-04', '2022-04-05', '2022-04-06', '2022-04-07',\n",
      "               '2022-04-08', '2022-04-11', '2022-04-12', '2022-04-13',\n",
      "               '2022-04-14', '2022-04-18'],\n",
      "              dtype='datetime64[ns]', name='Date', length=565, freq=None)\n"
     ]
    }
   ],
   "source": [
    "# train data : test data = 80 : 20\n",
    "train_index = int(dr.shape[0]*0.8)\n",
    "print(train_index)\n",
    "# 80:20 으로 train / test 분할 (numpy array로 변환)\n",
    "train_data = dr[:train_index].values\n",
    "train_date = dr[:train_index].index.values.astype('datetime64[D]')\n",
    "print(\"train data index\")\n",
    "print(dr[:train_index].index)\n",
    "test_data = dr[train_index:].values\n",
    "test_date = dr[train_index:].index.values.astype('datetime64[D]')\n",
    "print(\"test data index\")\n",
    "print(dr[train_index:].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2011-01-31', '2011-02-01', '2011-02-02', ..., '2020-01-16',\n",
       "       '2020-01-17', '2020-01-21'], dtype='datetime64[D]')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data 준비 - sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_window(data, window_size_past, window_size_future):\n",
    "    # sequential data와 과거데이터 수, 미래 데이터 수 를 받아서 과거데이터 ndarray, 미래데이터 ndarray를 반환한다.\n",
    "    inputs_past = []\n",
    "    inputs_future = []\n",
    "    # print(len(data)-window_size_past-window_size_future)\n",
    "    for i in range(len(data)-window_size_past-window_size_future):\n",
    "        # print(i)\n",
    "        inputs_past.append(data[i:i+window_size_past].copy())\n",
    "        inputs_future.append(data[i+window_size_past:i+window_size_past+window_size_future].copy())\n",
    "        \n",
    "    np_inputs_past = np.array(inputs_past)\n",
    "    np_inputs_future = np.array(inputs_future)\n",
    "    return np_inputs_past, np_inputs_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자산배분비율 산출 기준 : 과거 60일 데이터로 산출, 미래 20일의 최적수익율로 최적화\n",
    "window_size_past = 60\n",
    "window_size_future = 20\n",
    "\n",
    "xc_train, xf_train = make_data_window(train_data, window_size_past, window_size_future)\n",
    "xc_test, xf_test = make_data_window(test_data, window_size_past, window_size_future)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(485, 60, 15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xc_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2011-01-31', '2011-02-01', '2011-02-02', ..., '2019-09-23',\n",
       "       '2019-09-24', '2019-09-25'], dtype='datetime64[D]')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xc_train_date = train_date[:len(train_date)-window_size_past-window_size_future]\n",
    "xc_train_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xc_train.shape: (2178, 60, 15)\n",
      "xf_train.shape: (2178, 20, 15)\n",
      "xc_test.shape: (485, 60, 15)\n",
      "xf_test.shape: (485, 20, 15)\n"
     ]
    }
   ],
   "source": [
    "print(\"xc_train.shape:\",xc_train.shape)\n",
    "print(\"xf_train.shape:\",xf_train.shape)\n",
    "print(\"xc_test.shape:\",xc_test.shape)\n",
    "print(\"xf_test.shape:\",xf_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPN(Markowitz Portfolio Network) 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주요 상수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 월간 수익율 정도의 스케일로 변환한다.(아마퀀트님 코드)\n",
    "# scikit-learn 의 scaler 를 사용하는게 낫지 않을까?\n",
    "xc_train_scaled = xc_train.astype('float32') * 20.0\n",
    "xf_train_scaled = xf_train.astype('float32') * 20.0\n",
    "xc_test_scaled = xc_test.astype('float32') * 20.0\n",
    "xf_test_scaled = xf_test.astype('float32') * 20.0\n",
    "\n",
    "N_TIME = xc_train_scaled.shape[1]\n",
    "N_FUTURE = xf_train_scaled.shape[1]\n",
    "N_STOCKS = xf_train_scaled.shape[2]\n",
    "\n",
    "# 입력 순서에 따른 상관성을 제거하기 위해 sklearn.utils의 함수를 이용해서 shuffle을 수행한다.\n",
    "xc_train_scaled, xf_train_scaled = shuffle(xc_train_scaled, xf_train_scaled)\n",
    "\n",
    "# over confidence를 제어할 조절 변수 정의\n",
    "GAMMA_CONST = 0.1\n",
    "# GAMMA_CONST = 0.001\n",
    "REG_CONST = 0.1\n",
    "# REG_CONST = 0.001\n",
    "SAVE_MODEL = 'mpn.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 20)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_TIME, N_FUTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 목적함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markowitz_objective(y_true, y_pred):\n",
    "    W = y_pred\n",
    "    xf_rtn = y_true\n",
    "    W = tf.expand_dims(W, axis=1)\n",
    "    R = tf.expand_dims(tf.reduce_mean(xf_rtn, axis=1), axis=2)\n",
    "    C = tfp.stats.covariance(xf_rtn, sample_axis=1)\n",
    "    \n",
    "    rtn = tf.matmul(W, R)\n",
    "    vol = tf.matmul(W, tf.matmul(C, tf.transpose(W, perm=[0,2,1]))) * GAMMA_CONST\n",
    "    reg = tf.reduce_sum(tf.square(W), axis=-1) * REG_CONST\n",
    "    # print(f\"rtn: {rtn}, vol: {vol}, reg: {reg}\")\n",
    "    objective = rtn - vol - reg\n",
    "    \n",
    "    return -tf.reduce_mean(objective, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM으로 Markowitz 모델을 생성한다.\n",
    "xc_input = Input(batch_shape=(None, N_TIME, N_STOCKS))\n",
    "h_lstm = LSTM(64, dropout=0.5)(xc_input)\n",
    "y_output = Dense(N_STOCKS, activation='tanh')(h_lstm)\n",
    "\n",
    "# 특정 종목을 과도하게 매수하는 것을 방지하기 위해 위에서 tanh를 사용했다.(over confidence 방지용)\n",
    "# REG_CONST를 적용했기때문에 이미 고려된 사항이지만, 안전을 위해 추가했다.\n",
    "\n",
    "# 마코비츠의 최적 weights\n",
    "y_output = Activation('softmax')(y_output)\n",
    "\n",
    "model = Model(inputs=xc_input, outputs=y_output)\n",
    "model.compile(loss=markowitz_objective, optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MPN을 학습하고 결과를 저장한다.\n",
    "hist = model.fit(xc_train_scaled, xf_train_scaled, epochs=150, batch_size=32, validation_data=(xc_test_scaled, xf_test_scaled))\n",
    "model.save(SAVE_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습결과 확인(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss trajectory를 확인한다.\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(hist.history['loss'], label='train loss')\n",
    "plt.plot(hist.history['val_loss'], label='validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 최적 투자비율 추정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_w(n=0):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    y_pred = model.predict(xc_test[n].reshape(1, N_TIME, N_STOCKS))[0]\n",
    "    # plt.bar(np.arange(N_STOCKS), y_pred, alpha=0.7)\n",
    "    plt.bar(tickers, y_pred, alpha=0.7)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "check_w(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_w(310)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 Markowitz 모델을 가져온다.\n",
    "SAVE_MODEL = 'mpn.h5'\n",
    "model = load_model(SAVE_MODEL, compile = False)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 백 테스트를 수행한다.\n",
    "prt_value = [10000]   # portfolio의 초기 value\n",
    "crp_value = [10000]   # CRP의 초기 value\n",
    "spy_value = [10000]\n",
    "w_crp = np.ones(N_STOCKS) / N_STOCKS   # CRP 비율 (균등 비율)\n",
    "w_spy = np.zeros(15)\n",
    "w_spy[9] = 1\n",
    "\n",
    "w_history = []\n",
    "date_history = [test_date[0]] # 시작일을 저장한다.\n",
    "\n",
    "for i in range(0, xc_test_scaled.shape[0], N_FUTURE):\n",
    "    # 이 시점에 각 종목을 w_prt 비율대로 매수한다.\n",
    "    # 학습할 때 월간 수익률로 변환했으므로, 여기서도 변환해야 한다.\n",
    "    # x = xc_test[i][np.newaxis,:,:] * 20.0\n",
    "    x = xc_test_scaled[i][np.newaxis,:,:]\n",
    "    w_prt = model.predict(x)[0]\n",
    "    w_history.append(w_prt)\n",
    "    date_history.append(test_date[i+N_TIME])\n",
    "\n",
    "    # 다음 기의 누적 수익률\n",
    "    m_rtn = np.sum(xf_test[i], axis = 0)\n",
    "\n",
    "    # 누적 수익률과 w_prt (W)로 포트폴리오의 수익률을 계산한다.\n",
    "    prt_value.append(prt_value[-1] * np.exp(np.dot(w_prt, m_rtn)))\n",
    "    # print(f\"m_rtn: {m_rtn}\")\n",
    "    # print(f\"w_prt: {w_prt}\")\n",
    "    crp_value.append(crp_value[-1] * np.exp(np.dot(w_crp, m_rtn)))\n",
    "    spy_value.append(crp_value[-1] * np.exp(np.dot(w_spy, m_rtn)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_w_history = np.array(w_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=np_w_history, index=date_history[1:], columns=tickers).plot(figsize=(16,8), stacked=True, kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crp_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 평가 시점의 날짜를 발췌한다.\n",
    "# idx = np.arange(0, len(test_date), N_FUTURE)\n",
    "\n",
    "# Markowitz 성과와 CRP 성과를 데이터 프레임에 기록해 둔다.\n",
    "# perf_df = pd.DataFrame({'crp':crp_value, 'markowitz':prt_value}, index=test_date[idx])\n",
    "perf_df = pd.DataFrame({'crp':crp_value, 'markowitz':prt_value, 'spy':spy_value}, index=date_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_df.plot(figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38975e1155517b59f8c43fae6b2cd5fc2c10decd5084c524c2b39939c331f457"
  },
  "kernelspec": {
   "display_name": "aa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
